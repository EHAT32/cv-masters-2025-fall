{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2acb90",
   "metadata": {},
   "source": [
    "# Домашнее задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44580b20",
   "metadata": {},
   "source": [
    "Выполнил Козин Роман"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b149aa",
   "metadata": {},
   "source": [
    "# Часть 1. Классификатор 128×128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e621fc7",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846cad6",
   "metadata": {},
   "source": [
    "Для ускорения процесса обучения используем не весь датасет ImageNet, возьмём часть классов. Пусть будет 40 классов. Выберем их случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46317d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a8d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(52)\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382d452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n01882714', 'n02279972', 'n02699494', 'n02123394', 'n03026506', 'n01910747', 'n04532106', 'n07920052', 'n02058221', 'n07695742', 'n02268443', 'n07753592', 'n03160309', 'n04356056', 'n01784675', 'n02124075', 'n07871810', 'n04417672', 'n02927161', 'n03837869', 'n04540053', 'n09193705', 'n02364673', 'n02909870', 'n01774750', 'n12267677', 'n03404251', 'n01698640', 'n02988304', 'n01770393', 'n03930313', 'n02977058', 'n02963159', 'n01644900', 'n02415577', 'n03992509', 'n07875152', 'n02814533', 'n02395406', 'n02814860']\n"
     ]
    }
   ],
   "source": [
    "CLASS_NUM = 40\n",
    "classes_path = './data/tiny-imagenet-200/tiny-imagenet-200/wnids.txt'\n",
    "with open(classes_path) as f:\n",
    "    ids = f.read().split('\\n')\n",
    "selected_classes = random.sample(ids[:-1], CLASS_NUM)\n",
    "print(selected_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ebfa",
   "metadata": {},
   "source": [
    "На всякий случай зафиксируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13b900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = ['n01882714', 'n02279972', 'n02699494', 'n02123394', 'n03026506', 'n01910747', 'n04532106', 'n07920052', \n",
    "                    'n02058221', 'n07695742', 'n02268443', 'n07753592', 'n03160309', 'n04356056', 'n01784675', 'n02124075', \n",
    "                    'n07871810', 'n04417672', 'n02927161', 'n03837869', 'n04540053', 'n09193705', 'n02364673', 'n02909870', \n",
    "                    'n01774750', 'n12267677', 'n03404251', 'n01698640', 'n02988304', 'n01770393', 'n03930313', 'n02977058', \n",
    "                    'n02963159', 'n01644900', 'n02415577', 'n03992509', 'n07875152', 'n02814533', 'n02395406', 'n02814860']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafee215",
   "metadata": {},
   "source": [
    "Возьмём ту же реализацию датасета из прошлого ДЗ, останется только добавить resize в изображения. Делать дополнительной стратификации для разделения датасета на обучение и валидацию не нужно, в ImageNet датасет уже разбит на нужные выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9fed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class SmallDataset(Dataset):\n",
    "    def __init__(self, root: str, classes: list[str], ds_type: str, device = torch.device(\"cpu\"), transform = None) -> None:\n",
    "        super().__init__()\n",
    "        self.ds_type = ds_type\n",
    "        self.root = root\n",
    "        self.classes = classes\n",
    "        if ds_type == 'val':\n",
    "            self.prepareValData()\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "        self.label_to_idx = {label : idx for idx, label in enumerate(classes)}\n",
    "        \n",
    "    def prepareValData(self) -> None:\n",
    "        self.valData = []\n",
    "        labelsSet = set(self.classes)\n",
    "        with open(os.path.join(self.root, self.ds_type, \"val_annotations.txt\")) as f:\n",
    "            rows = f.readlines()\n",
    "            f.close()    \n",
    "        for row in rows:\n",
    "            data = self._parseLine(row)\n",
    "            label = data[1]\n",
    "            if label in labelsSet:\n",
    "                self.valData.append(tuple(data))\n",
    "    \n",
    "    def _parseLine(self, row: str):\n",
    "        els = row.split(\"\\t\")\n",
    "        data = [els[k].strip(\"\\n\") for k in range(len(els))]\n",
    "        if self.ds_type == 'val':\n",
    "            start_idx = 2\n",
    "        else:\n",
    "            start_idx = 1\n",
    "        for i in range(start_idx, len(data)):\n",
    "            data[i] = int(data[i])\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        if self.ds_type == 'val':\n",
    "            data = self.valData[index]\n",
    "            img_path = os.path.join(self.root, self.ds_type, \"images\", data[0])\n",
    "            label = data[1]\n",
    "            bbox = tuple(data[2:])\n",
    "        else:\n",
    "            label_idx = index // 500\n",
    "            img_idx = index % 500\n",
    "            label = self.classes[label_idx]\n",
    "            label_root = os.path.join(self.root, self.ds_type, label)\n",
    "            img_path = os.path.join(label_root, \"images\", f'{label}_{img_idx}.JPEG')\n",
    "            with open(os.path.join(label_root, f\"{label}_boxes.txt\")) as f:\n",
    "                rows = f.readlines()\n",
    "                f.close()\n",
    "            row = rows[img_idx]\n",
    "            data = self._parseLine(row)\n",
    "            bbox = tuple(data[1:])\n",
    "        \n",
    "        bbox = torch.tensor(bbox).to(self.device)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return (img.to(self.device), torch.tensor(self.label_to_idx[label]).to(self.device), bbox)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        #структура исходного датасета такова, что на каждый класс приходится 500 изображений\n",
    "        if self.ds_type == 'train':\n",
    "            return len(self.classes) * 500\n",
    "        else:\n",
    "            return len(self.valData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8dcb45",
   "metadata": {},
   "source": [
    "Увеличим размер изображений до 128x128 при помощи ```transforms.Resize()```, используем стандартную билинейную интерполяцию. Также в тренировку добавим несколько аугментаций для более стабильного обучения. Добавим аугментации, предложенные в задании: RandomResizedCrop, Flip, Rotation.\n",
    "\n",
    "В RandomResizedCrop выставим адекватные размеры обрезки, чтобы не оставлять в датасете слишком маленькие участки исходного изображения.\n",
    "\n",
    "В RandomRotation используем повороты до 30 градусов, чтобы тоже слишком сильно не искажать изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054c23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomResizedCrop((128, 128), (0.6, 1)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], #найденные в интернете параметры для imagenet\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], #найденные в интернете параметры для imagenet\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_root = \"./data/tiny-imagenet-200/tiny-imagenet-200\"\n",
    "train_data = SmallDataset(root=data_root, classes=selected_classes,\n",
    "                          ds_type='train', transform=train_transforms, device=device)\n",
    "val_data = SmallDataset(root=data_root, classes=selected_classes,\n",
    "                        ds_type='val', transform=transforms_list, device=device)\n",
    "train_data_loader = DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24264aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "img, label, bbox = train_data[0]\n",
    "\n",
    "print(img.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
