{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2acb90",
   "metadata": {},
   "source": [
    "# Домашнее задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44580b20",
   "metadata": {},
   "source": [
    "Выполнил Козин Роман"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b149aa",
   "metadata": {},
   "source": [
    "# Часть 1. Классификатор 128×128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e621fc7",
   "metadata": {},
   "source": [
    "## Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846cad6",
   "metadata": {},
   "source": [
    "Для ускорения процесса обучения используем не весь датасет ImageNet, возьмём часть классов. Пусть будет 40 классов. Выберем их случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46317d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a8d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(52)\n",
    "torch.manual_seed(52)\n",
    "np.random.seed(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382d452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n01882714', 'n02279972', 'n02699494', 'n02123394', 'n03026506', 'n01910747', 'n04532106', 'n07920052', 'n02058221', 'n07695742', 'n02268443', 'n07753592', 'n03160309', 'n04356056', 'n01784675', 'n02124075', 'n07871810', 'n04417672', 'n02927161', 'n03837869', 'n04540053', 'n09193705', 'n02364673', 'n02909870', 'n01774750', 'n12267677', 'n03404251', 'n01698640', 'n02988304', 'n01770393', 'n03930313', 'n02977058', 'n02963159', 'n01644900', 'n02415577', 'n03992509', 'n07875152', 'n02814533', 'n02395406', 'n02814860']\n"
     ]
    }
   ],
   "source": [
    "CLASS_NUM = 40\n",
    "classes_path = './data/tiny-imagenet-200/tiny-imagenet-200/wnids.txt'\n",
    "with open(classes_path) as f:\n",
    "    ids = f.read().split('\\n')\n",
    "selected_classes = random.sample(ids[:-1], CLASS_NUM)\n",
    "print(selected_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ebfa",
   "metadata": {},
   "source": [
    "На всякий случай зафиксируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13b900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = ['n01882714', 'n02279972', 'n02699494', 'n02123394', 'n03026506', 'n01910747', 'n04532106', 'n07920052', \n",
    "                    'n02058221', 'n07695742', 'n02268443', 'n07753592', 'n03160309', 'n04356056', 'n01784675', 'n02124075', \n",
    "                    'n07871810', 'n04417672', 'n02927161', 'n03837869', 'n04540053', 'n09193705', 'n02364673', 'n02909870', \n",
    "                    'n01774750', 'n12267677', 'n03404251', 'n01698640', 'n02988304', 'n01770393', 'n03930313', 'n02977058', \n",
    "                    'n02963159', 'n01644900', 'n02415577', 'n03992509', 'n07875152', 'n02814533', 'n02395406', 'n02814860']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafee215",
   "metadata": {},
   "source": [
    "Возьмём ту же реализацию датасета из прошлого ДЗ, останется только добавить resize в изображения. Делать дополнительной стратификации для разделения датасета на обучение и валидацию не нужно, в ImageNet датасет уже разбит на нужные выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9fed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class SmallDataset(Dataset):\n",
    "    def __init__(self, root: str, classes: list[str], ds_type: str, device = torch.device(\"cpu\"), transform = None) -> None:\n",
    "        super().__init__()\n",
    "        self.ds_type = ds_type\n",
    "        self.root = root\n",
    "        self.classes = classes\n",
    "        if ds_type == 'val':\n",
    "            self.prepareValData()\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "        self.label_to_idx = {label : idx for idx, label in enumerate(classes)}\n",
    "        \n",
    "    def prepareValData(self) -> None:\n",
    "        self.valData = []\n",
    "        labelsSet = set(self.classes)\n",
    "        with open(os.path.join(self.root, self.ds_type, \"val_annotations.txt\")) as f:\n",
    "            rows = f.readlines()\n",
    "            f.close()    \n",
    "        for row in rows:\n",
    "            data = self._parseLine(row)\n",
    "            label = data[1]\n",
    "            if label in labelsSet:\n",
    "                self.valData.append(tuple(data))\n",
    "    \n",
    "    def _parseLine(self, row: str):\n",
    "        els = row.split(\"\\t\")\n",
    "        data = [els[k].strip(\"\\n\") for k in range(len(els))]\n",
    "        if self.ds_type == 'val':\n",
    "            start_idx = 2\n",
    "        else:\n",
    "            start_idx = 1\n",
    "        for i in range(start_idx, len(data)):\n",
    "            data[i] = int(data[i])\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        if self.ds_type == 'val':\n",
    "            data = self.valData[index]\n",
    "            img_path = os.path.join(self.root, self.ds_type, \"images\", data[0])\n",
    "            label = data[1]\n",
    "            bbox = tuple(data[2:])\n",
    "        else:\n",
    "            label_idx = index // 500\n",
    "            img_idx = index % 500\n",
    "            label = self.classes[label_idx]\n",
    "            label_root = os.path.join(self.root, self.ds_type, label)\n",
    "            img_path = os.path.join(label_root, \"images\", f'{label}_{img_idx}.JPEG')\n",
    "            with open(os.path.join(label_root, f\"{label}_boxes.txt\")) as f:\n",
    "                rows = f.readlines()\n",
    "                f.close()\n",
    "            row = rows[img_idx]\n",
    "            data = self._parseLine(row)\n",
    "            bbox = tuple(data[1:])\n",
    "        \n",
    "        bbox = torch.tensor(bbox).to(self.device)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return (img.to(self.device), torch.tensor(self.label_to_idx[label]).to(self.device), bbox)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        #структура исходного датасета такова, что на каждый класс приходится 500 изображений\n",
    "        if self.ds_type == 'train':\n",
    "            return len(self.classes) * 500\n",
    "        else:\n",
    "            return len(self.valData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8dcb45",
   "metadata": {},
   "source": [
    "Увеличим размер изображений до 128x128 при помощи ```transforms.Resize()```, используем стандартную билинейную интерполяцию. Также в тренировку добавим несколько аугментаций для более стабильного обучения. Добавим аугментации, предложенные в задании: RandomResizedCrop, Flip, Rotation.\n",
    "\n",
    "В RandomResizedCrop выставим адекватные размеры обрезки, чтобы не оставлять в датасете слишком маленькие участки исходного изображения.\n",
    "\n",
    "В RandomRotation используем повороты до 30 градусов, чтобы тоже слишком сильно не искажать изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomResizedCrop((128, 128), (0.6, 1)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], #найденные в интернете параметры для imagenet\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], #найденные в интернете параметры для imagenet\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_root = \"./data/tiny-imagenet-200/tiny-imagenet-200\"\n",
    "train_data = SmallDataset(root=data_root, classes=selected_classes,\n",
    "                          ds_type='train', transform=train_transforms, device=device)\n",
    "val_data = SmallDataset(root=data_root, classes=selected_classes,\n",
    "                        ds_type='val', transform=val_transforms, device=device)\n",
    "train_data_loader = DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24264aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "img, label, bbox = train_data[0]\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daed8d",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f67d59",
   "metadata": {},
   "source": [
    "Сперва ради эксперимента попробуем обучить модель из прошлого дз и посмотрим на её точность. Используем тот же код для обучения, также будем использовать AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train_one_run(model, optimizer, criterion, train_data_loader, val_data_loader, epoch_num=20):\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = None\n",
    "    train_acc = None\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        train_correct = 0\n",
    "        train_predicted = 0\n",
    "        model.train()\n",
    "        \n",
    "        for images, labels, _ in tqdm(desc=f\"Training epoch {epoch + 1}\", iterable=train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = logits.argmax(dim=-1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_predicted += labels.size(0)\n",
    "        \n",
    "        train_loss_history.append(loss.item())\n",
    "        train_acc_history.append(train_correct / train_predicted)\n",
    "\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_predicted = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in tqdm(iterable=val_data_loader, desc=f\"Validating\"):\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_predicted += labels.size(0)\n",
    "            \n",
    "            val_loss_history.append(loss.item())\n",
    "            val_acc = val_correct / val_predicted\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = deepcopy(model.state_dict())\n",
    "            train_acc = train_correct / train_predicted\n",
    "\n",
    "    return {\n",
    "        \"best_state_dict\": best_state_dict,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"best_train_acc\" : train_acc,\n",
    "        \"train_loss_history\": train_loss_history,\n",
    "        \"val_loss_history\": val_loss_history,\n",
    "        \"train_acc_history\": train_acc_history,\n",
    "        \"val_acc_history\": val_acc_history,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_multiple_runs(runs, model_constructor, optimizer_constructor, criterion, train_data_loader, val_data_loader, epoch_num=20):\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_train_accs = []\n",
    "    all_val_accs = []\n",
    "\n",
    "    best_overall_train_acc = None\n",
    "    best_overall_val_acc = 0.0\n",
    "    best_overall_state_dict = None\n",
    "\n",
    "    for run in range(runs):\n",
    "        print(f\"\\n===== Run {run + 1}/{runs} =====\")\n",
    "        model = model_constructor()\n",
    "        optimizer = optimizer_constructor(model)\n",
    "\n",
    "        result = train_one_run(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_data_loader=train_data_loader,\n",
    "            val_data_loader=val_data_loader,\n",
    "            epoch_num=epoch_num,\n",
    "        )\n",
    "\n",
    "        all_train_losses.append(result[\"train_loss_history\"])\n",
    "        all_val_losses.append(result[\"val_loss_history\"])\n",
    "        all_train_accs.append(result[\"train_acc_history\"])\n",
    "        all_val_accs.append(result[\"val_acc_history\"])\n",
    "\n",
    "        if result[\"best_val_acc\"] > best_overall_val_acc:\n",
    "            best_overall_val_acc = result[\"best_val_acc\"]\n",
    "            best_overall_state_dict = deepcopy(result[\"best_state_dict\"])\n",
    "            best_overall_train_acc = result[\"best_train_acc\"]\n",
    "\n",
    "    mean_train_losses = np.mean(np.array(all_train_losses), axis=0)\n",
    "    mean_val_losses = np.mean(np.array(all_val_losses), axis=0)\n",
    "    mean_train_accs = np.mean(np.array(all_train_accs), axis=0)\n",
    "    mean_val_accs = np.mean(np.array(all_val_accs), axis=0)\n",
    "\n",
    "    best_model = model_constructor()\n",
    "    best_model.load_state_dict(best_overall_state_dict)\n",
    "\n",
    "    return {\n",
    "        \"best_model\": best_model,                    \n",
    "        \"best_val_acc\": best_overall_val_acc,\n",
    "        \"best_train_acc\": best_overall_train_acc,        \n",
    "        \"mean_train_losses\": mean_train_losses,      \n",
    "        \"mean_val_losses\": mean_val_losses,\n",
    "        \"mean_train_accs\": mean_train_accs,\n",
    "        \"mean_val_accs\": mean_val_accs,\n",
    "        \"all_histories\": {\n",
    "            \"train_losses\": all_train_losses,\n",
    "            \"val_losses\": all_val_losses,\n",
    "            \"train_accs\": all_train_accs,\n",
    "            \"val_accs\": all_val_accs,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f06a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, activation = nn.ReLU(), *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels or stride > 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.activation = activation\n",
    "            \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        res = self.conv1(x)\n",
    "        res = self.conv2(res)\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(identity)\n",
    "        res = res + identity\n",
    "        res = self.activation(res)\n",
    "        return res\n",
    "\n",
    "class ShallowLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer = BasicResBlock(in_channels, out_channels, stride=2, activation=activation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ShallowResNet(nn.Module):\n",
    "    def __init__(self, activation, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer1 = ShallowLayer(in_channels=3, out_channels=64, activation=activation) #64, 32, 32\n",
    "        self.layer2 = ShallowLayer(in_channels=64, out_channels=128, activation=activation) #128 16, 16\n",
    "        self.layer3 = ShallowLayer(in_channels=128, out_channels=256, activation=activation) # 256, 8, 8\n",
    "        self.avg = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.fc = nn.Linear(in_features=256, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.avg(self.layer3(x))\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89d0c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline():\n",
    "    return ShallowResNet(nn.ELU())\n",
    "\n",
    "def init_AdamW(model):\n",
    "    return torch.optim.AdamW(model.parameters(), lr=1e-3, \n",
    "                             weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2591e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsFinal = train_multiple_runs(\n",
    "    runs=3,\n",
    "    model_constructor=make_baseline,\n",
    "    optimizer_constructor=init_AdamW,\n",
    "    criterion=criterion,\n",
    "    train_data_loader=train_data_loader,\n",
    "    val_data_loader=val_data_loader,\n",
    "    epoch_num=40,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
